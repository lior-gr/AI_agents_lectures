"""Minimal agent loop with OpenAI Chat Completions and stubbed tool calls.

Authentication note:
- The OpenAI SDK reads the API key from the `OPENAI_API_KEY` environment variable
  when `OpenAI()` is created without an explicit `api_key` argument.
"""

from __future__ import annotations

import logging
import os

from openai import OpenAI

from mcp_client import MCPClient
from tools import TOOL_SCHEMAS

# Basic logger used to show progress for each step in the loop.
LOGGER = logging.getLogger(__name__)

# Keep the model configurable via environment variable.
DEFAULT_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

# Hard step limit requested for this minimal agent.
DEFAULT_MAX_STEPS = 5

# Per-call token cap requested in the prompt.
MAX_TOKENS_PER_CALL = 800

# Use a low temperature for more deterministic outputs.
TEMPERATURE = 0.2

# Keep agent behavior intentionally simple: execute tools to satisfy the goal, then stop.
SYSTEM_PROMPT = (
    "You are a minimal task agent. "
    "Use tools to do task operations. "
    "Do not plan; do not explain long strategies. "
    "For requests like 'add X and then show tasks', call add_task first, then list_tasks. "
    "Do not edit JSON directly. "
    "Once the goal is satisfied, return a short final answer."
)


def run_agent(goal: str, *, model: str | None = None, max_steps: int = DEFAULT_MAX_STEPS) -> str:
    """Run a minimal agent loop for the given user goal."""
    if not goal.strip():
        raise ValueError("goal must be a non-empty string")

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
    )

    # API key source: this client uses OPENAI_API_KEY from environment by default.
    client = OpenAI()
    chosen_model = model or DEFAULT_MODEL
    mcp_client = MCPClient()
    mcp_client.start()

    # Message history list: this is the full conversation state sent on every model call.
    messages: list[dict] = [
        {
            "role": "system",
            "content": SYSTEM_PROMPT,
        },
        {"role": "user", "content": goal},
    ]

    # AGENT LOOP:
    # This loop is unchanged: same step control, history updates, and stopping conditions.
    # Only the tool execution transport changed (direct call -> MCP client request).
    try:
        for step in range(1, max_steps + 1):
            LOGGER.info("Step %s/%s: requesting model response", step, max_steps)

            response = client.chat.completions.create(
                model=chosen_model,
                messages=messages,
                # Tool schemas are defined in tools.py so the agent does not own task logic.
                tools=TOOL_SCHEMAS,
                tool_choice="auto",
                temperature=TEMPERATURE,
                # Token guardrail: each call is capped at 800 tokens as requested.
                max_tokens=MAX_TOKENS_PER_CALL,
            )

            assistant_message = response.choices[0].message
            tool_calls = assistant_message.tool_calls or []
            assistant_text = assistant_message.content or ""

            # Keep the assistant message in history exactly as returned so the next step has full context.
            messages.append(assistant_message.model_dump(exclude_none=True))

            # TOOL CALL PROCESSING:
            # If the assistant asks for tools, we execute stub handlers and append tool outputs.
            if tool_calls:
                LOGGER.info("Step %s: processing %s tool call(s)", step, len(tool_calls))
                for call in tool_calls:
                    tool_name = call.function.name
                    tool_args = call.function.arguments or ""
                    LOGGER.info("Tool call id=%s name=%s", call.id, tool_name)

                    # Execution layer only: send tool calls through MCP transport.
                    # The agent remains unchanged as an orchestrator (loop/history/stop logic).
                    try:
                        # Argument parsing/normalization is handled inside MCPClient,
                        # so agent code stays focused on loop orchestration only.
                        tool_response = mcp_client.request(tool_name, tool_args)
                        if tool_response.get("status") == "ok":
                            tool_result = str(tool_response.get("result", "OK"))
                        else:
                            tool_result = str(tool_response.get("error", "Unknown tool error"))
                    except Exception as exc:  # pragma: no cover - defensive guard for runtime tool errors
                        tool_result = f"Error executing tool '{tool_name}': {exc}"

                    messages.append(
                        {
                            # This marks the message as tool output.
                            "role": "tool",
                            # Must match the tool call id generated by the model.
                            "tool_call_id": call.id,
                            # Tool name for readability/debugging.
                            "name": tool_name,
                            # The actual tool result sent back to the model.
                            "content": tool_result,
                        }
                    )
                continue

            # STOPPING CONDITIONS:
            # Condition 1: assistant returned a normal answer with no tool calls, so we can stop early.
            if assistant_text.strip():
                LOGGER.info("Stopping at step %s: final answer received", step)
                return assistant_text

            # Condition 2: no tool calls and empty output; stop to avoid useless iterations.
            LOGGER.info("Stopping at step %s: empty response with no tool calls", step)
            return "No final answer produced."

        # Condition 3: hard cap reached (max_steps=5 by default).
        LOGGER.info("Stopping: reached max_steps=%s", max_steps)
        return f"Stopped after reaching max_steps={max_steps}."
    finally:
        mcp_client.close()
